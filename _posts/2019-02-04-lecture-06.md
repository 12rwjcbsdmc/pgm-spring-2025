---
layout: distill
title: "Lecture 6: Learning Partially Observed GM and the Expectation-Maximization Algorithm"
description: Introduction to the process of estimating the parameters of graphical models from data using the E-M (Baum-Welch) algorithm.
date: 2019-02-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Siddharth Dalmia
    url: "#"
  - name: Benjamin Striner
    url: "#"
  - name:  Hillary Wehry # author's full name
    url: "#"

editors:
  - name: Paul Liang  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction
In this previous lectures, we introduced the concept of learning Graphical Models from data, where *learning* is the process of estimating the parameters, and in some cases, the network structure from the data.
Lecture 5 introduced this concept in the setting of *completely observed* GMs. Maximum likelihood estimation (MLE) in the setting of fully observed nodes  and globally independent parameters for each conditional probability distribution is straightforward because the likelihood function can be fully decomposed as a product of independent terms, one for each conditional probability distribution in the network. This means that we can maximize each local likelihood function independently of the rest of the network, and then combine the solutions to get an MLE solution.

In this lecture, we turn our attention to **partially observed graphical models**, an equally important class of models including Hidden Markov Models and Gaussian Mixture Models. As we will see, in the presence of partially observed data, we lose important properties of the likelihood function: its unimodality, closed-form representation, and decomposition as a product of likelihoods for the different parameters. As a result the learning problem becomes substantially more complex, and we turn to the Expectation-Maximization algorithm to enable estimation of our model parameters.

## Partially Observed GMs
### Directed but partially observed GM
First consider the case of a fully observed, directed graphical model:
<figure>
<div class="row">
<div class="col one">
<img src="{{ '/assets/img/notes/lecture-06/lec6_fullobserved.png' | relative_url }}" />
</div>
</div>
<figcaption>
<strong>Fully observed, directed graphical model</strong>. In fully observed, i.i.d. settings,The log-likelihood function decomposes into a sum of local terms. We can maximize each local likelihood function independently.
</figcaption>
</figure>
$$ l_c (\theta; D) = \text{log } p(x,z | \theta) = \text{log } p(z| \theta_z) + \text{log } p(x | z, \theta_x) $$

Compare this to the case of directed, but partially observed GMs. Suppose we now do not observe one of the variables, yet we would still like to write down the likelihood of the data. To do this, we marginalize (i.e. integrate or sum out) the unobserved probability.
<figure>
<div class="row">
<div class="col one">
<img src="{{ '/assets/img/notes/lecture-06/lec6_partialobserved.png' | relative_url }}" />
</div>
</div>
<figcaption>
<strong>Partially observed, directed graphical model</strong>. With unobserved or latent variables, the log-likelihood function no longer decomposes into a sum of local terms. All the parameters become coupled together via marginalization.
</figcaption>
</figure>
$$ l_c (\theta; D) = \text{log } \sum_z p(x,z | \theta) = \text{log } \sum_z p(z| \theta_z) p(x | z, \theta_x) $$

#### Unobserved variables:
+ A variable can be unobserved or *latent* because it is a(n):
1. **Abstract or imaginary quantity** meant to simplifiy the data generation process, e.g. speech recognition models, mixture models.
2. **A real-world object that is difficult or impossible to measure**, e.g. the temperature of a star, causes of disease, evolutionary ancestors.
3. **A real-world object that was not measured** due to missed samples, e.g. faulty sensors.
+ Discrete latent variables can used to partition or cluster data into sub-groups
+ Continuous latent variables (factors) can be used for dimensionality reduction (e.g. factor analysis, etc)

#### Example: HMMs for Speech Recognition  
Our phones are capable of recognizing speech patterns and converting them to text. The initial approaches to this problem were based on Hidden Markov Models (HMMs). We assume there is a latent state that generates the noisy signal of speech that can be "chunked" into different components, or phonemes. We create dictionaries of different phonemes for different languages, and then we try to infer the sequence of phonemes that generated the speech.
<figure>
<div class="row">
<div class="col one">
<img src="{{ '/assets/img/notes/lecture-06/lec6_speechHMM.png' | relative_url }}" />
</div>
</div>
<figcaption>
<strong>An HMM used for speech recognition is a real-world example of a partially observed, directed graphical model</strong>. We try to infer the sequence of phonemes tat generated the noisy signal of speech.
</figcaption>
</figure>

#### Example: A Baysian Network for Biological Evolution
<figure>
<div class="row">
<div class="col one">
<img src="{{ '/assets/img/notes/lecture-06/lec6_bioEvol.png' | relative_url }}" />
</div>
</div>
<figcaption>
<strong> This Baysian Network is not full observable </strong>. We cannot measure data from evolutionary ancestors no longer on earth, but they are still a useful latent variable for the model.
</figcaption>
</figure>

### Probabilistic Inference
A GM $M$ describes a unique probability distribution $P$.  To learn partially observable GMs, we must iterate between inferring relationships described by $P$ and estimating a plausible model $M$ from data $D$. In this way *inference* can be considered a subroutine of the learning problem:

**Task 1.** How do we answer queries about $P$, e.g. $P(X|Y)$? 
- *We use inference as a name for the process of computing answers to these queries.*

**Task 2.** How do we estimate a plausible model $M$ from data $D$?
- *We use learning as a name for the process of a obtaining point estimate of $M$*. 
*For the Bayesian approach, we seek $P(M|D)$ , which is also an inference problem.*

There are many approaches for inference in GMs. They can be divided into two classes:
+ **Exact inference algorithms.** Including the elimination algorithm, message-passing algorithm (sum-product, belief propagation), the junction tree algorithms. These algorithms can give the precise result of query. The major topic of this lecture is on exact inference algorithms.  
+ **Approximate inference techniques.** Including stochastic simulation / sampling methods, Markov chain Monte Carlo (MCMC) methods, variational algorithms. These algorithms only gives an approximate answer to the inference query. We will cover these methods in future lectures.


### Mixture Models

A density model $p(x)$ may be multi-modal, but we may be able to model it as a mixture of uni-modal distributions (e.g. Gaussians). Suppose we have a dataset $x_n \in \mathbb{R}^D$ where $n=1$,...,$N$, and we believe the data has been generated from a mixture of $K$ Gaussian components.  Let $z \in \{1,...,K \}$ be an *unobserved* random variable, such that $P(z = k) = \pi_k$, where $k = 1,...,K$. For $P(z)$ to be a valid probability distribution, $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^K \pi_k =1$. We call $\pi_k$ the *mixture proportion*. Our assumption of the Gaussian components given a mixture label $k$ is expressed as follows: $ P(x| z=k) = N(x |\mu_k, \Sigma_k)$, where $\mu_k$ and $\Sigma_k$ are the parameters for each Gaussian component. Thus,

$$ 
p(x_n) = \sum_z P(x | z) P(z) = \sum_{k=1}^K N(x | \mu_k, \Sigma_k) \pi_k 
$$

+ $Z$ is a latent class indicator vector:

$$ 
p(z_n) = \text{multi}(z_n | \pi) = \prod_k (\pi_k)^{z_n^k} 
$$

+ $X$ is a conditional Gaussian variable with a class-specific mean/covariance:

$$ 
p(x_n | z_n^k = 1, \mu, \Sigma) = \frac{1}{(2\pi)^{m/2} |\Sigma_k|^{1/2}} \text{exp} (-\frac{1}{2} (x_n - \mu_k)^T \Sigma_k^{-1} (x_n - \mu_k))
$$

<figure>
<div class="row">
<div class="col one">
<img src="{{ '/assets/img/notes/lecture-06/lec6_GMMs.png' | relative_url }}" />
</div>
</div>
<figcaption>
<strong>A Gaussian Mixture Model is an example of a partially observed GM</strong>. A latent variable Z is introduced as a class indicator to separate a multi-modal distribution into simpler unimodal distributions.
</figcaption>
</figure>

+ The likelihood of a sample:

<d-math block>
\begin{aligned}
p(x_n | \mu, \Sigma) &= \sum_k p(z^k = 1 | \pi) p(x | z^k = 1, \mu, \Sigma) \\
&= \sum_{z_n} \prod_k ((\pi_k)^{z_n^k} N(x_n | \mu_k, \Sigma_k)^{z_n^k}) \\
&= \sum_k \pi_k N(x | \mu_k, \Sigma_k)
\end{aligned}
</d-math>

#### MLE Solution for a **Fully-Observed** Gaussian Mixture Model 
+ The data log-likelihood can be decomposed when our latent variable $Z$ is also observed:

<d-math block>
\begin{aligned}
l (\theta ; D) &= \text{log} \prod_n p(z_n , x_n) = \text{log} \prod_n p(z_n | \pi) p(x_n | z_n , \mu, \sigma_k)^{z_n^k} \\
&= \sum_n \text{log} \pi_k ^{z_n^k} + \sum_n \text{log} \prod_k N(x_n | \mu_k, \sigma_k)^{z_n^k} \\
&= \sum_n \sum_k z_n^k \text{log} \pi_k - \sum_n \sum_k z_n^k \frac{1}{2 \sigma_k^2} (x_n - \mu_k)^2 + C
\end{aligned}
</d-math>

+ Thus the MLE solution for the parameters can be found separately for these parameters:

<d-math block>
\begin{aligned}
\hat{\pi}_{k\text{, MLE}} &= \text{arg max}_{\pi} l(\theta | D) \\
\hat{\mu}_{k \text{, MLE}} &= \text{arg max}_{\mu} l(\theta | D) = \frac{\sum_n z_n^k x_n}{\sum_n z_n^k} \\
\hat{\sigma}_{k \text{, MLE}} &= \text{arg max}_{\sigma} l(\theta | D) \\
\end{aligned}
</d-math>


### Why we need EM (known vs. unknown iteration)

## E-M

### Example:  K-means vs. GMMs  (Hillary) 

### Complete and Incomplete Log Likelihood

#### Complete Log Likelihood

Complete Log Likelihood is likelihood if both $x$ and $z$ are observed. Optimization given both $x$ and $z$ is straightforward MLE and can decompose into terms that can be independently maximized.

$$
\ell_c (\theta; x,z) \vcentcolon= \log p(x,z \mid \theta) = \log p(x \mid \theta_x, z) + \log p(z \mid \theta_z)
$$

#### Incomplete Log Likelihood

If $z$ is unobserved, we marginalize over $z$. We can no longer decouple into independent terms.

$$
\ell_c(\theta;x) \vcentcolon= \log p(x \mid \theta) = \log \sum_{z} p(x,z \mid \theta)
$$

### Expected Complete Log Likelihood

Define the *expected complete log likelihood* for any distribution $z \sim q$. This function is a linear combination of $\ell_c$.

$$
\langle \ell_c(\theta; x,z)\rangle_q 
\vcentcolon= \sum_z q(z \mid x, \theta) \log p(x,z \mid \theta)
= \mathbb{E}_{q(z \mid x, \theta)} \log p(x,z \mid \theta)
$$

The expected complete log likelihood can be used to create a lower bound on the incomplete log likelihood. The proof uses Jensen's inequality ($\log \mathbb{E}[x] \ge \mathbb{E}[ \log x]$). The proof also uses the importance sampling trick ($\mathbb{E}_p[ f(x)] = \mathbb{E}_q[ \frac{f(x)p(x)}{q(x)}] $).

$$
\begin{aligned}
\ell(\theta, x) &= \log p(x \mid \theta) \\
&= \log \sum_z p(x,z \mid \theta) \\
&= \log \sum_z \frac{q(z \mid x)}{q(z \mid x)} p(x,z \mid \theta) & \text{importance sampling trick}\\
&= \log \sum_z q(z \mid x) \frac{p(x,z \mid \theta) }{q(z \mid x)} & \log \mathbb{E}_q[\ldots]\\
&\ge \sum_z q(z \mid x) \log \frac{p(x,z \mid \theta)}{q(z \mid x)} & \text{Jensen's } \mathbb{E}_q [\log \ldots] \\
&= \mathbb{E}_q [\log p(x,z \mid \theta)] - \mathbb{E}_q \log q(z \mid x)\\
\ell(\theta, x) &\ge  \langle \ell_c(\theta; x,z) \rangle_q + H_q
\end{aligned}
$$

The second term $H_q$ is the entropy of $q(z \mid x)$. A distribution is $[0,1]$, so the log of a distribution is $\langle -\infty,0 \rangle$, which is why the constant $H_q$ is positive. Because $H_q$ is positive and does not depend on $\theta$, we can try to maximize $\langle \ell_c(\theta; x,z)\rangle_q$ to maximize a lower bound on $\ell(\theta, x)$.

### Lower Bounds and Free Energy

For fixed data $x$ define a functional called the free energy. This is the term $\langle\ell_c(\theta; x,z)\rangle_q + H_q$ from the above proof.

$$
F(q,\theta) \vcentcolon= \sum_z q(z \mid x) \log \frac{p(x,z \mid \theta)}{q(z \mid x)} \le \ell(\theta; x)
$$

We can perform *EM algorithm* (generally MM algorithm) on $F$:

- E-step: $q^{t+1}=\operatorname{argmax}_q F(q, \theta^t)$
- M-step: $\theta^{t+1}=\operatorname{argmax}_\theta F(q^t, \theta)$

#### E-step

The E-step is a maximization over the posterior over latent variables given data and parameters $q(z \mid x, \theta)$.

$$q^{t+1}=\operatorname{argmax}_q F(q, \theta^t)$$

If $q$ is the optimal posterior $p(z \mid \theta^t, x)$, this maximization attains the bound $\ell(\theta^t; x)$. The proof uses an application of the Bayes rule $p(x,z \mid \theta^t)=p(x \mid \theta^t) p(z \mid x, \theta^t)$.

$$
\begin{aligned}
F(p(z\mid x,\theta^t), \theta^t)
&= \sum_z p(z \mid x, \theta^t) \log \frac{p(x,z \mid \theta^t)}{p(z \mid x, \theta^t)}\\
&= \sum_z p(z \mid x, \theta^t) \log \frac{p(x \mid \theta^t) p(z \mid x, \theta^t)}{p(z \mid x, \theta^t)} & \text{Bayes rule}\\
&= \sum_z p(z \mid x, \theta^t) \log p(x \mid \theta^t) \\
&= \log p(x \mid \theta^t) \sum_z p(z \mid x, \theta^t) & \text{term does not depend on $z$}\\
&= \log p(x \mid \theta^t) & \text{sum of distribution is 1}\\
&= \ell(\theta^t; x)
\end{aligned}
$$

#### M-step

The M-step is a maximization over the parameters given data and latent variables. As discussed previously, the free energy breaks into two terms, one of which ($H_q$) does not depend on $\theta$. Therefore we only need to consider the first term during the M-step.

$$
F(q,\theta) = \langle \ell_c(\theta;x,z)\rangle_q + H_q 
$$

$$
\theta^{t+1}=\operatorname{argmax}_\theta F(q^t, \theta) =\operatorname{argmax}_{\theta}  \langle \ell_c(\theta;x,z)\rangle_{q^t}
$$

If $q^t = p(z \mid x, \theta)$, this is equivalent to fully observable MLE where statistics are replaced by their expectations w.r.t. $ p(z \mid x, \theta)$. We are minimizing the expected loss over the distribution $p$.



### Example: EM for general BNs  (Siddharth)

### Example: EM for  conditional mixture models (Siddharth)

### Example: HMM & Baum Welch (Siddharth)

## Summary
