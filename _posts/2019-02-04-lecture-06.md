---
layout: distill
title: "Lecture 6: Learning Partially Observed GM: the Expectation-Maximization Algorithm"
description: Introduction to the process of estimating the parameters of graphical models from data using the E-M (Baum-Welch) algorithm.
date: 2019-02-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Hillary Wehry # author's full name; let's order by sections
    url: "#"  # optional URL to the author's homepage
  - name: Siddharth Dalmia
    url: "#"
  - name: Benjamin Striner
    url: "#"
  - name: 
    url: "#"

editors:
  - name: Paul Liang  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Introduction
**old content** In this previous lectures, we introduced the concept of learning **Graphical Models** from data.
Now we know that we can use a graphical model $M$ (Bayesian network or undirected graph model) to specify a probability distribution $P_{M}$ satisfying some conditional independence property.
In this lecture, we will study how to utilize a graphical model. Given a GM $M$, we generally have two type of tasks
+ **Inference:** answering queries about the probability distribution $P_M$ defined by $M$, for examples, <script type="math/tex">P_M(X|Y)</script> where $X$ and $Y$ are subsets of variables in GM $M$.
+ **Learning:** estimating a plausible model $M$ from data $D$.  We call the process of obtaining a point estimate of $M$ as *learning*, but for Bayesian, they seek the posterior distribution of <script type="math/tex">p(M|D)</script>, which is actually an *inference* problem. The learning task is highly related to the inference task. When we want to compute a point estimate of $M$, we need to do inference to impute the missing data if not all the variables are observable. So the learning algorithm usually uses inference as a subroutine.

### Key Points Learning fully Observed GMs
 
### MLE for CPD

### MLE for BNs

### Plates 

## Partially Observed GMs

### Example: HMMs, Bio evolution, 

### Probabilistic Inference

### Why we need EM (known vs. unknown iteration)

## E-M

### Example:  K-means vs. GMMs  (Hillary) 

### Complete and Incomplete Log Likelihood

#### Complete Log Likelihood

Complete Log Likelihood is likelihood if both $x$ and $z$ are observed. Optimization given both $x$ and $z$ is straightforward MLE and can decompose into terms that can be independently maximized.

$$
\ell_c (\theta; x,z) \vcentcolon= \log p(x,z \mid \theta) = \log p(x \mid \theta_x, z) + \log p(z \mid \theta_z)
$$

#### Incomplete Log Likelihood

If $z$ is unobserved, we marginalize over $z$. We can no longer decouple into independent terms.

$$
\ell_c(\theta;x) \vcentcolon= \log p(x \mid \theta) = \log \sum_{z} p(x,z \mid \theta)
$$

### Expected Complete Log Likelihood

Define the *expected complete log likelihood* for any distribution $z \sim q$. This function is a linear combination of $\ell_c$.

$$
\langle \ell_c(\theta; x,z)\rangle_q 
\vcentcolon= \sum_z q(z \mid x, \theta) \log p(x,z \mid \theta)
= \mathbb{E}_{q(z \mid x, \theta)} \log p(x,z \mid \theta)
$$

The expected complete log likelihood can be used to create a lower bound on the incomplete log likelihood. The proof uses Jensen's inequality ($\log \mathbb{E}[x] \ge \mathbb{E}[ \log x]$). The proof also uses the importance sampling trick ($\mathbb{E}_p[ f(x)] = \mathbb{E}_q[ \frac{f(x)p(x)}{q(x)}] $).

$$
\begin{aligned}
\ell(\theta, x) &= \log p(x \mid \theta) \\
&= \log \sum_z p(x,z \mid \theta) \\
&= \log \sum_z \frac{q(z \mid x)}{q(z \mid x)} p(x,z \mid \theta) & \text{importance sampling trick}\\
&= \log \sum_z q(z \mid x) \frac{p(x,z \mid \theta) }{q(z \mid x)} & \log \mathbb{E}_q[\ldots]\\
&\ge \sum_z q(z \mid x) \log \frac{p(x,z \mid \theta)}{q(z \mid x)} & \text{Jensen's } \mathbb{E}_q [\log \ldots] \\
&= \mathbb{E}_q [\log p(x,z \mid \theta)] - \mathbb{E}_q \log q(z \mid x)\\
\ell(\theta, x) &\ge  \langle \ell_c(\theta; x,z) \rangle_q + H_q
\end{aligned}
$$

The second term $H_q$ is the entropy of $q(z \mid x)$. A distribution is $[0,1]$, so the log of a distribution is $\langle -\infty,0 \rangle$, which is why the constant $H_q$ is positive. Because $H_q$ is positive and does not depend on $\theta$, we can try to maximize $\langle \ell_c(\theta; x,z)\rangle_q$ to maximize a lower bound on $\ell(\theta, x)$.

### Lower Bounds and Free Energy

For fixed data $x$ define a functional called the free energy. This is the term $\langle\ell_c(\theta; x,z)\rangle_q + H_q$ from the above proof.

$$
F(q,\theta) \vcentcolon= \sum_z q(z \mid x) \log \frac{p(x,z \mid \theta)}{q(z \mid x)} \le \ell(\theta; x)
$$

We can perform *EM algorithm* (generally MM algorithm) on $F$:

- E-step: $q^{t+1}=\operatorname{argmax}_q F(q, \theta^t)$
- M-step: $\theta^{t+1}=\operatorname{argmax}_\theta F(q^t, \theta)$

#### E-step

The E-step is a maximization over the posterior over latent variables given data and parameters $q(z \mid x, \theta)$.

$$q^{t+1}=\operatorname{argmax}_q F(q, \theta^t)$$

If $q$ is the optimal posterior $p(z \mid \theta^t, x)$, this maximization attains the bound $\ell(\theta^t; x)$. The proof uses an application of the Bayes rule $p(x,z \mid \theta^t)=p(x \mid \theta^t) p(z \mid x, \theta^t)$.

$$
\begin{aligned}
F(p(z\mid x,\theta^t), \theta^t)
&= \sum_z p(z \mid x, \theta^t) \log \frac{p(x,z \mid \theta^t)}{p(z \mid x, \theta^t)}\\
&= \sum_z p(z \mid x, \theta^t) \log \frac{p(x \mid \theta^t) p(z \mid x, \theta^t)}{p(z \mid x, \theta^t)} & \text{Bayes rule}\\
&= \sum_z p(z \mid x, \theta^t) \log p(x \mid \theta^t) \\
&= \log p(x \mid \theta^t) \sum_z p(z \mid x, \theta^t) & \text{term does not depend on $z$}\\
&= \log p(x \mid \theta^t) & \text{sum of distribution is 1}\\
&= \ell(\theta^t; x)
\end{aligned}
$$

#### M-step

The M-step is a maximization over the parameters given data and latent variables. As discussed previously, the free energy breaks into two terms, one of which ($H_q$) does not depend on $\theta$. Therefore we only need to consider the first term during the M-step.

$$
F(q,\theta) = \langle \ell_c(\theta;x,z)\rangle_q + H_q 
$$

$$
\theta^{t+1}=\operatorname{argmax}_\theta F(q^t, \theta) =\operatorname{argmax}_{\theta}  \langle \ell_c(\theta;x,z)\rangle_{q^t}
$$

If $q^t = p(z \mid x, \theta)$, this is equivalent to fully observable MLE where statistics are replaced by their expectations w.r.t. $ p(z \mid x, \theta)$. We are minimizing the expected loss over the distribution $p$.



### Example: EM for general BNs  (Siddharth)

### Example: EM for  conditional mixture models (Siddharth)

### Example: HMM & Baum Welch (Siddharth)

## Summary
