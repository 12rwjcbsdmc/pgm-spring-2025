---
layout: distill
title: "Lecture 13: Approximate Inference: Monte Carlo and Sequential Monte Carlo Methods"
description: Wrapping up variational inference, and overview of Monte Carlo methods.

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Helen Zhou
    url: "#"
  - name: Ziyang Wang
    url: "#"
  - name: Yufeng Shen
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

---

<!-- Helen -->

## Recap of Variational Principles

Last class, we reformulated probabilistic inference as an optimization problem. Invoking conjugate duality, we defined the exact variational formulation for the partition function: 
$$A(\theta) = \sup_{\mu \in \mathcal{M}}\left\{ \theta^T \mu - A^*(\mu) \right\}$$

where:
* $A(\theta)$ is the partition function
* $\theta$ corresponds to the canonical parameters in an exponential family distribution
* $\mu$ corresponds to the mean parameters
* $\mathcal{M}$ is the marginal polytope (convex combination of the marginals of the sufficient statistics)
* $A^*$ is the negative entropy function

### Example: Two-node Ising Model

To better understand this variational formulation, we consider the two-node Ising model.

<img src="{{ '/assets/img/notes/lecture-13/two_node_ising.png' | relative_url }}" style="width: 30%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

Its distribution is

$$p(x;\theta) \propto \exp\{\theta_1 x_1 + \theta_2 x_2 + \theta_{12}x_{12}\}$$

and it has sufficient statistics $$\phi(x) = \left \{ x_1, x_2, x_1 x_2 \right \}$$.

Plugging into the variational formulation of the partition function, we get 

$$A(\theta) = \max_{\{\mu_1, \mu_2, \mu_{12}\in \mathcal{M}\}}
\left\{ \theta_1 \mu_1 + \theta_2 \mu_{2} + \theta_{12} \mu_{12} - A^*(\mu) \right\}$$

where the marginal polytope $\mathcal{M}$ is defined in terms of half-spaces:
<d-math block>
\begin{aligned}
\mu_1 &\geq \mu_{12}\\
\mu_2 &\geq \mu_{12}\\
\mu_{12} &\geq 0\\
1 + \mu_{12} &\geq \mu_1 + \mu_2
\end{aligned}
</d-math>

As we showed in the previous lecture, the dual $A^*$ can be computed as the 
negative entropy of the model:

$$A^*(\mu) = \mu_{12}\log\mu_{12} + (\mu_1 - \mu_{12})\log(\mu_1 - \mu_{12}) + 
(\mu_{2} - \mu_{12})\log(\mu_2 - \mu_{12}) + (1 + \mu_{12} - \mu_1 - \mu_2)
\log(1 + \mu_{12} - \mu_1 - \mu_2)$$

By plugging in for $A^*$, taking the derivatives of the objective function, and setting to zero, we
can solve the variational problem and arrive at the optima. For instance,

$$\mu_1(\theta) = \frac{\exp(\theta_1) + \exp(\theta_1 + \theta_2 + \theta_{12})}
{1 + \exp(\theta_1) + \exp(\theta_2) + \exp(\theta_1 + \theta_2 + \theta_{12})}$$

In this example, we were able to compute everything exactly. However this is not always
possible, and so approximations such as the mean field method, Bethe approximation,
and loopy belief propagation are used. The mean field method (to be described
more in the following section) gives a non-convex inner bound and exact form of 
entropy. The non-convex Bethe approximation and loopy belief propagation 
(discussed later in these notes) provide polyhedral outer bounds.

## Mean Field Approximation

Graphically, the mean field approximation can be thought of as a tractable 
subgraph approximation $F$ to the original full graph $G$. Consider a graph with $x_i$'s
for each node and canonical parameters defined on each edge. The space of these
parameters $\theta$ is such that the partition function $A(\theta)$ is bounded.
If there is no edge connecting two nodes $x_i$and $x_j$, that is, $\theta_{ij} = 0$,
then we know that the mean parameters are related such that $\mu_{ij} = P(x_i, x_j) = P(x_i)P(x_j) = \mu_{i}\mu_{j}$.

### Mean Field Methods

For a given tractable subgraph $F$, a **subset** of canonical parameters is

$$\mathcal{M}(F; \phi) := \{ \tau \in \mathcal{R}^d | \tau = \mathcal{E}_\theta [\phi(X)] \text{ for some } \theta \in \Omega(F)\}$$

This subset of parameters constrained by the subgraph defines a new subspace, known as
the **inner approximation** to the full marginal polytope. That is,

$$\mathcal{M}(F;\phi)^o \subseteq \mathcal{M}(G;\phi)^o$$.

Making the mean field approximation, we solve the relaxed problem
<d-math block>
\max_{\tau \in \mathcal{M}_F(G)}\left \{\left \langle \tau, \theta \right \rangle - A_F^*(\tau) \}\right \}
</d-math>

where 

$$A_F^* = A^*|_{\mathcal{M}_F (G)}$$ 

is the exact dual function restricted to $$\mathcal{M}_F(G)$$.

### Naive Mean Field for Ising Model

Consider the Ising model in its $\left \{0,1\right \}$ representation:

$$p(x) \propto \exp\left\{ \sum_{s\in V}{x_s \theta_s} + \sum_{(s,t)\in E}{x_s x_t \theta_{st}} \right\}$$

In making the mean field approximation, we cut out all of the edges as shown below:

<img src="{{ '/assets/img/notes/lecture-13/ising_mean_field.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

The mean parameters are:

$$\mu_s = \mathbb{E}_p [X_s] = P(X_s = 1)$$

for all $s\in V$, and

$$\mu_{st} = \mathbb{E}_p [X_s X_t] = P[(X_s, X_t) = (1,1)]$$

for all $(s,t) \in E$. 

For the fully disconnected graph $F$,

$$\mathcal{M}_F(G) = \left\{ \tau \in \mathbb{R}^{|V| + |E|} | 0 \leq \tau_s \leq 1, \forall s \in V, \tau_{st} = \tau_s \tau_t, \forall (s,t) \in E \right\}$$

Note that by the mean field approximation, we have that $\tau_{st} = \tau_s \tau_t$. 
The dual decomposes into a sum, one term for each node:

$$A_F^*(\tau) = \sum_{s \in V}[\tau_s \log \tau_s + (1-\tau_s) \log(1 - \tau_s)]$$

The mean field objective, which lower bounds $A(\theta)$, is as stated below.
 
$$A(\theta) \geq \max_{(\tau_1, ..., \tau_m) \in [0,1]^m} \left\{ \sum_{s \in V}{\theta_s \tau_s} + \sum_{(s,t) \in E}{\theta{st}\tau_s\tau_t - A_F^*(\tau)} \right\}$$

To solve, take the derivative with respect to $\tau$ to arrive at the naive mean field
update equations:

$$\tau \leftarrow \sigma\left ( \theta_s + \sum_{t \in N(s)} \theta_s \tau_t \right )$$

### Summary of Mean Field Approximation

* Mean field optimization is always non-convex for any exponential family in which the state space $\mathcal{X}^m$ is finite. Thus, it is not guaranteed to get the global optimum.
* Recall that the marginal polytope is a convex hull, and that the adjusted marginal under
the mean field approximation contains all the extreme points. Note that if this adjusted
marginal is a strict subset, then it must be non-convex.
* Simple algorithm, but solves a much more complex intractable problem in an iterative fashion.

<!-- Ziyang -->

## Bethe Approximation and Sum-Product

### Sum-Product Algorithm Recap

Message passing rule:
$$
M_{ts}(x_s) \leftarrow \kappa\sum_{x_t'} \bigg\{ \psi_{st}(x_s, x_t')\psi_t(x_t')\prod_{u\in N(t)/s}M_{ut}(x_t') \bigg\}
$$

Marginals:
$$
\mu_{s}(x_s) = \kappa \psi_{s}(x_s)\prod_{t\in N(s)}M^{*}_{ts}(x_s)
$$
### Trees Graphic Models

We have discrete variables $X_s \in \{0, 1, ..., m_s - 1\}$ on a tree $T = (V, E)$.

The sufficient statistics are $\mathbb{I}_j(x_s)$ where $s \in V, j \in \chi_s$ and $\mathbb{I}_{jk}(x_s, x_t)$ where $(s, t)\in E, (j, k)\in \chi_s \times \chi_t$

Then the mean parameters are marginals and pair-wise marginals:
$$
\mu_s(x_s) = \mathbb{P}(X_s=x), \ \mu_{st}(x_s, x_t) = \mathbb{P}(X_s=x_s, X_t=x_t)
$$
### Marginal Polytopes for Trees

The algorithm produces exact solution for tree graphic models. The marginal polytope for is the same as the true polytope, because the local consistency is sufficient for global consistency in a tree.
$$
\mathcal{M}(T) = \bigg\{ \mu \ge 0 | \sum_{x_s}\mu(x_s) = 1, \sum_{x_t}\mu_{st}(x_s, x_t) = \mu(x_s) \bigg\}
$$
$\mu \in \mathcal{M}(T) \Rightarrow p_\mu(x):=\prod_{s\in V}\mu_s(x_s)\prod_{(s, t)\in E} \frac{\mu_{st}(x_s, x_t)}{\mu_s(x_s)\mu_t(x_t)}$

### Decomposition of Entropy for Trees

In order to perform optimization, we define $A^*(\mu) = - H(p(x;\mu))$

The entropy can be decomposed as:
\begin{align}
H(p(x;\mu)) 
    &\begin{aligned}
        = - \sum_xp(x;\mu)logp(x;\mu)
    \end{aligned}\\
    &\begin{aligned}
        =\sum_{s\in V}\bigg(-\sum_{x_s}\mu_s(x_s)log\mu_s(x_s)\bigg) + \sum_{(s, t)\in E}\bigg( -\sum_{x_s, x_t}\mu_{st}(x_s, x_t)log\frac{\mu_{st}(x_s, x_t)}{\mu_s(x_s)\mu_t(x_t)} \bigg)
    \end{aligned}\\
    &\begin{aligned}
        = \sum_{s\in V}H_s(\mu_s) - \sum_{(s, t)\in E}I_{st}(\mu_{st})
    \end{aligned}
\end{align}

### Exact Variational Inference on Trees

With $\mu$ as local parameters satisfying both local and global consistency, we use Lagrangian to solve the problem below:

$$
A(\theta) = \texttt{max}_{\mu \in \mathcal{M}(T)} \bigg\{ \langle \theta ,\mu \rangle - A^*(\mu)\bigg\} = \texttt{max}_{\mu \in \mathcal{M}(T)} \bigg\{ \langle \theta ,\mu \rangle +\sum_{s\in V}H_s(\mu_s) -\sum_{(s, t)\in E}I_{st}(\mu_{st}) \bigg\}
$$

Assign Lagrange multiplier $\lambda_{ss}$ for $C_{ss}(\mu) := 1 - \sum_{x_s}\mu(x_s) = 0$, and $\lambda_{ts}(x_s)$ for $C_{ts}(x_s;\mu) := \mu(x_s) - \sum_{x_t}\mu_{st}(x_s, x_t) = 0$

$$
\mathcal{L}(\mu, \lambda) = \langle \theta, \mu \rangle + \sum_{s\in V}H_s(\mu_s) -\sum_{(s, t)\in E}I_{st}(\mu_{st})
$$

$$
\frac{\delta\mathcal{L}}{\delta\mu_s(x_s)} = \theta_s(x_s) - log\mu_s(x_s) + \sum_{t \in N(s)} \lambda_{ts}(x_s) + C
$$

$$
\frac{\delta\mathcal{L}}{\delta\mu_{st}(x_s, x_t)} = \theta_st(x_s, x_t) - log \frac{\mu_{st}(x_s, x_t)}{\mu_s(x_s)\mu_t(x_t)} - \lambda_{ts}(x_s) - \lambda_{st}(x_t) + C'
$$

Setting the derivatives to 0 and solve for $\mu$

$$
\mu_s(x_s) \propto \texttt{exp}\{\theta_s(x_s)\}\prod_{t\in N(s)}\texttt{exp}\{\lambda_{ts}(x_s)\}
$$

$$
\mu_s(x_s, x_t) \propto \texttt{exp}\{\theta_s(x_s)+ \theta_t(x_t) + \theta_{st}(x_s, x_t)\}\prod_{u\in N(s)/t}\texttt{exp}\{\lambda_{us}(x_s)\}\prod_{v\in N(t)/s}\texttt{exp}\{\lambda_{vs}(x_t)\}
$$

After adjusting the Lagrange multipliers to enforce constraints, $\mu$ correspond to cluster message and singleton message. We conclude that the message passing updates are a Lagrange method to solve the stationary condition of the variational formulation.

$$
M_{ts}(x_s) \leftarrow \sum_{x_t}  \texttt{exp}\{\theta_t(x_t) + \theta_{st}(x_s, x_t)\}\prod_{u\in N(t)/s}M_{ut}(x_t)
$$


<!-- Yufeng -->

## Belief Propagation on Arbitrary Graphs and Bethe Approximation
Inspired by sum-product algorithm on tree graph, we can have another approximation approach to solve the variational formulation:

$$A(\theta)=\sup_{\mu\in \mathcal{M}}\{\theta^T\mu-A^*(\mu)\}$$

The two main difficulties of above variational problem are:

 * The marginal polytope $\mathcal{M}$ is hard to characterize.
 * The exact entropy $-A^*(\mu)$ lacks explicit form.

To address the first difficulty, we use the tree-based outer bounder:

<d-math block>
\mathcal{L}(G) = \left \{ \tau\geq 0 | \sum_{x_s} \tau_s(x_s)=1,\sum_{x_t}\tau_{st}(x_s,x_t)=\tau_s(x_s)\right \}
</d-math>

The conditions on the $\tau$ are the locally consistent conditions, and we don't care about whether they are globally consistent or not. There are some other global constraint for general graphs. So, $\mathcal{L}(G)\supseteq\mathcal{M}(G)$,which means that it is an outer bounder. Since the number of faces of $\mathcal{L}(G)$ growth linearly with the size of the graph, it is easier to characterize.
For the second  difficulty, we can approximate the true entropy with Bethe entropy, which is the exact expression for trees:

$$-A^*(\tau)\approx H_{Bethe}(\tau):=\sum_{s\in V} H_s(\tau_s)-\sum_{(s,t)\in E}I_{st}(\tau_{st})$$.

It has the explicit form, which is the sum of entropy of every node minus the sum of mutual information of every edge.

Combine these two approximations, we derived the Bethe Variational Problem (BVP):

$$A_{Bethe}(\theta)= \sup_{\tau\in \mathcal{L}(G)}\{\theta^T\tau +\sum_{s\in V} H_s(\tau_s)-\sum_{(s,t)\in E}I_{st}(\tau_{st})\}$$.

In contrast to Mean Field Method, which uses an inner approximation $\mathcal{M}_F(G)$ for $\mathcal{M}(G)$ and the exact dual function on $\mathcal{M}_F(G)$, the BVP uses an outer bounder $\mathcal{L}(G)$ for the marginal polytope and an approximated entropy. If the graph is a tree, the two approximations above are actually exact, so the BVP are exact for trees.

Some notes about BVP:

 * It may not converge. (Because the objective function is not convex)
 * It may not converge to the right answer. (Because the solution may not be in the $\mathcal{M}(G)$)
 * There is no guarantee that $A_{Bethe}(\theta)$ is the lower bound of $A(\theta)$
 
 In summary:
 
  * Variational methods in general turn inference into an optimization problem via exponential families and convex duality.
  * The exact variational principle is intractable to solve. There are two distinct components for approximation:
     1.  Inner or outer bound for the marginal polytope
     2. Various approximation for the entropy function
  * Three approximated approaches:
     1. Mean field: non-convex inner bound of marginal polytope and exact form of entropy
     2. Bethe Variational Problem: polyhedral outer bound and non-convex Bethe approximation of entropy.
     3. Kikuchi: tighter polyhedral outer bounds and better entropy.
approximations
 
 

